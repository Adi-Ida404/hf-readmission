{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3f7feb",
   "metadata": {},
   "source": [
    "# Import Patient Data Tables\n",
    "We will import all patient-related CSV files from the `/raw/readmission/` folder using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497237b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "data_path = '../raw/readmission/'\n",
    "admissions = pd.read_csv(data_path + 'admissions_202208161605.csv')\n",
    "cptevents = pd.read_csv(data_path + 'cptevents_202208161605.csv')\n",
    "d_labitems = pd.read_csv(data_path + 'd_labitems_202208161605.csv')\n",
    "diagnoses_icd = pd.read_csv(data_path + 'diagnoses_icd_202208161605.csv')\n",
    "drgcodes = pd.read_csv(data_path + 'drgcodes_202208161605.csv')\n",
    "labevents = pd.read_csv(data_path + 'labevents_202208161605.csv')\n",
    "patients = pd.read_csv(data_path + 'patients_202208161605.csv')\n",
    "procedures_icd = pd.read_csv(data_path + 'procedures_icd_202208161605.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4084afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admissions: (58976, 19)\n",
      "Index(['row_id', 'subject_id', 'hadm_id', 'admittime', 'dischtime',\n",
      "       'deathtime', 'admission_type', 'admission_location',\n",
      "       'discharge_location', 'insurance', 'language', 'religion',\n",
      "       'marital_status', 'ethnicity', 'edregtime', 'edouttime', 'diagnosis',\n",
      "       'hospital_expire_flag', 'has_chartevents_data'],\n",
      "      dtype='object')\n",
      "   row_id  subject_id  hadm_id                admittime  \\\n",
      "0      21          22   165315  2196-04-09 12:26:00.000   \n",
      "1      22          23   152223  2153-09-03 07:15:00.000   \n",
      "2      23          23   124321  2157-10-18 19:34:00.000   \n",
      "3      24          24   161859  2139-06-06 16:14:00.000   \n",
      "4      25          25   129635  2160-11-02 02:06:00.000   \n",
      "\n",
      "                 dischtime deathtime admission_type  \\\n",
      "0  2196-04-10 15:54:00.000       NaN      EMERGENCY   \n",
      "1  2153-09-08 19:10:00.000       NaN       ELECTIVE   \n",
      "2  2157-10-25 14:00:00.000       NaN      EMERGENCY   \n",
      "3  2139-06-09 12:48:00.000       NaN      EMERGENCY   \n",
      "4  2160-11-05 14:55:00.000       NaN      EMERGENCY   \n",
      "\n",
      "          admission_location         discharge_location insurance language  \\\n",
      "0       EMERGENCY ROOM ADMIT  DISC-TRAN CANCER/CHLDRN H   Private      NaN   \n",
      "1  PHYS REFERRAL/NORMAL DELI           HOME HEALTH CARE  Medicare      NaN   \n",
      "2  TRANSFER FROM HOSP/EXTRAM           HOME HEALTH CARE  Medicare     ENGL   \n",
      "3  TRANSFER FROM HOSP/EXTRAM                       HOME   Private      NaN   \n",
      "4       EMERGENCY ROOM ADMIT                       HOME   Private      NaN   \n",
      "\n",
      "            religion marital_status ethnicity                edregtime  \\\n",
      "0       UNOBTAINABLE        MARRIED     WHITE  2196-04-09 10:06:00.000   \n",
      "1           CATHOLIC        MARRIED     WHITE                      NaN   \n",
      "2           CATHOLIC        MARRIED     WHITE                      NaN   \n",
      "3  PROTESTANT QUAKER         SINGLE     WHITE                      NaN   \n",
      "4       UNOBTAINABLE        MARRIED     WHITE  2160-11-02 01:01:00.000   \n",
      "\n",
      "                 edouttime                                          diagnosis  \\\n",
      "0  2196-04-09 13:24:00.000                            BENZODIAZEPINE OVERDOSE   \n",
      "1                      NaN  CORONARY ARTERY DISEASE\\CORONARY ARTERY BYPASS...   \n",
      "2                      NaN                                         BRAIN MASS   \n",
      "3                      NaN                     INTERIOR MYOCARDIAL INFARCTION   \n",
      "4  2160-11-02 04:27:00.000                            ACUTE CORONARY SYNDROME   \n",
      "\n",
      "   hospital_expire_flag  has_chartevents_data  \n",
      "0                     0                     1  \n",
      "1                     0                     1  \n",
      "2                     0                     1  \n",
      "3                     0                     1  \n",
      "4                     0                     1  \n"
     ]
    }
   ],
   "source": [
    "# Display the shape of each DataFrame\n",
    "print('admissions:', admissions.shape)\n",
    "print(admissions.columns)\n",
    "print(admissions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7929207b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procedures_icd: (240095, 5)\n",
      "Index(['row_id', 'subject_id', 'hadm_id', 'seq_num', 'icd9_code'], dtype='object')\n",
      "   row_id  subject_id  hadm_id  seq_num  icd9_code\n",
      "0     944       62641   154460        3       3404\n",
      "1     945        2592   130856        1       9671\n",
      "2     946        2592   130856        2       3893\n",
      "3     947       55357   119355        1       9672\n",
      "4     948       55357   119355        2        331\n"
     ]
    }
   ],
   "source": [
    "print('procedures_icd:', procedures_icd.shape)\n",
    "print(procedures_icd.columns)\n",
    "print(procedures_icd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f772e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patients: (46520, 8)\n",
      "Index(['row_id', 'subject_id', 'gender', 'dob', 'dod', 'dod_hosp', 'dod_ssn',\n",
      "       'expire_flag'],\n",
      "      dtype='object')\n",
      "   row_id  subject_id gender                      dob  \\\n",
      "0     234         249      F  2075-03-13 00:00:00.000   \n",
      "1     235         250      F  2164-12-27 00:00:00.000   \n",
      "2     236         251      M  2090-03-15 00:00:00.000   \n",
      "3     237         252      M  2078-03-06 00:00:00.000   \n",
      "4     238         253      F  2089-11-26 00:00:00.000   \n",
      "\n",
      "                       dod                 dod_hosp dod_ssn  expire_flag  \n",
      "0                      NaN                      NaN     NaN            0  \n",
      "1  2188-11-22 00:00:00.000  2188-11-22 00:00:00.000     NaN            1  \n",
      "2                      NaN                      NaN     NaN            0  \n",
      "3                      NaN                      NaN     NaN            0  \n",
      "4                      NaN                      NaN     NaN            0  \n"
     ]
    }
   ],
   "source": [
    "print('patients:', patients.shape)\n",
    "print(patients.columns)\n",
    "print(patients.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "500d01ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labevents: (27854055, 9)\n",
      "Index(['row_id', 'subject_id', 'hadm_id', 'itemid', 'charttime', 'value',\n",
      "       'valuenum', 'valueuom', 'flag'],\n",
      "      dtype='object')\n",
      "   row_id  subject_id  hadm_id  itemid                charttime value  \\\n",
      "0     281           3      NaN   50820  2101-10-12 16:07:00.000  7.39   \n",
      "1     282           3      NaN   50800  2101-10-12 18:17:00.000   ART   \n",
      "2     283           3      NaN   50802  2101-10-12 18:17:00.000    -1   \n",
      "3     284           3      NaN   50804  2101-10-12 18:17:00.000    22   \n",
      "4     285           3      NaN   50808  2101-10-12 18:17:00.000  0.93   \n",
      "\n",
      "   valuenum valueuom      flag  \n",
      "0      7.39    units       NaN  \n",
      "1       NaN      NaN       NaN  \n",
      "2     -1.00    mEq/L       NaN  \n",
      "3     22.00    mEq/L       NaN  \n",
      "4      0.93   mmol/L  abnormal  \n"
     ]
    }
   ],
   "source": [
    "print('labevents:', labevents.shape)\n",
    "print(labevents.columns)\n",
    "print(labevents.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eb146b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drgcodes: (125557, 8)\n",
      "Index(['row_id', 'subject_id', 'hadm_id', 'drg_type', 'drg_code',\n",
      "       'description', 'drg_severity', 'drg_mortality'],\n",
      "      dtype='object')\n",
      "   row_id  subject_id  hadm_id drg_type  drg_code  \\\n",
      "0     342        2491   144486     HCFA        28   \n",
      "1     343       24958   162910     HCFA       110   \n",
      "2     344       18325   153751     HCFA       390   \n",
      "3     345       17887   182692     HCFA        14   \n",
      "4     346       11113   157980     HCFA       390   \n",
      "\n",
      "                                         description  drg_severity  \\\n",
      "0  TRAUMATIC STUPOR & COMA, COMA <1 HR AGE >17 WI...           NaN   \n",
      "1  MAJOR CARDIOVASCULAR PROCEDURES WITH COMPLICAT...           NaN   \n",
      "2            NEONATE WITH OTHER SIGNIFICANT PROBLEMS           NaN   \n",
      "3  SPECIFIC CEREBROVASCULAR DISORDERS EXCEPT TRAN...           NaN   \n",
      "4            NEONATE WITH OTHER SIGNIFICANT PROBLEMS           NaN   \n",
      "\n",
      "   drg_mortality  \n",
      "0            NaN  \n",
      "1            NaN  \n",
      "2            NaN  \n",
      "3            NaN  \n",
      "4            NaN  \n"
     ]
    }
   ],
   "source": [
    "print('drgcodes:', drgcodes.shape)\n",
    "print(drgcodes.columns)\n",
    "print(drgcodes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d184bd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cptevents: (573146, 12)\n",
      "Index(['row_id', 'subject_id', 'hadm_id', 'costcenter', 'chartdate', 'cpt_cd',\n",
      "       'cpt_number', 'cpt_suffix', 'ticket_id_seq', 'sectionheader',\n",
      "       'subsectionheader', 'description'],\n",
      "      dtype='object')\n",
      "   row_id  subject_id  hadm_id costcenter chartdate cpt_cd  cpt_number  \\\n",
      "0     317       11743   129545        ICU       NaN  99232     99232.0   \n",
      "1     318       11743   129545        ICU       NaN  99232     99232.0   \n",
      "2     319       11743   129545        ICU       NaN  99232     99232.0   \n",
      "3     320       11743   129545        ICU       NaN  99232     99232.0   \n",
      "4     321        6185   183725        ICU       NaN  99223     99223.0   \n",
      "\n",
      "  cpt_suffix  ticket_id_seq              sectionheader  \\\n",
      "0        NaN            6.0  Evaluation and management   \n",
      "1        NaN            7.0  Evaluation and management   \n",
      "2        NaN            8.0  Evaluation and management   \n",
      "3        NaN            9.0  Evaluation and management   \n",
      "4        NaN            1.0  Evaluation and management   \n",
      "\n",
      "              subsectionheader description  \n",
      "0  Hospital inpatient services         NaN  \n",
      "1  Hospital inpatient services         NaN  \n",
      "2  Hospital inpatient services         NaN  \n",
      "3  Hospital inpatient services         NaN  \n",
      "4  Hospital inpatient services         NaN  \n"
     ]
    }
   ],
   "source": [
    "print('cptevents:', cptevents.shape)\n",
    "print(cptevents.columns)\n",
    "print(cptevents.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "282d5689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_labitems: (753, 6)\n",
      "Index(['row_id', 'itemid', 'label', 'fluid', 'category', 'loinc_code'], dtype='object')\n",
      "   row_id  itemid                       label                      fluid  \\\n",
      "0     546   51346                      Blasts  Cerebrospinal Fluid (CSF)   \n",
      "1     547   51347                 Eosinophils  Cerebrospinal Fluid (CSF)   \n",
      "2     548   51348             Hematocrit, CSF  Cerebrospinal Fluid (CSF)   \n",
      "3     549   51349  Hypersegmented Neutrophils  Cerebrospinal Fluid (CSF)   \n",
      "4     550   51350           Immunophenotyping  Cerebrospinal Fluid (CSF)   \n",
      "\n",
      "     category loinc_code  \n",
      "0  Hematology    26447-3  \n",
      "1  Hematology    26451-5  \n",
      "2  Hematology    30398-2  \n",
      "3  Hematology    26506-6  \n",
      "4  Hematology        NaN  \n"
     ]
    }
   ],
   "source": [
    "print('d_labitems:', d_labitems.shape)\n",
    "print(d_labitems.columns)\n",
    "print(d_labitems.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f3cbcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diagnoses_icd: (651047, 5)\n",
      "Index(['row_id', 'subject_id', 'hadm_id', 'seq_num', 'icd9_code'], dtype='object')\n",
      "   row_id  subject_id  hadm_id  seq_num icd9_code\n",
      "0    1297         109   172335      1.0     40301\n",
      "1    1298         109   172335      2.0       486\n",
      "2    1299         109   172335      3.0     58281\n",
      "3    1300         109   172335      4.0      5855\n",
      "4    1301         109   172335      5.0      4254\n"
     ]
    }
   ],
   "source": [
    "print('diagnoses_icd:', diagnoses_icd.shape)\n",
    "print(diagnoses_icd.columns)\n",
    "print(diagnoses_icd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15fdcc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admissions: Index(['row_id', 'subject_id', 'hadm_id', 'admittime', 'dischtime',\n",
      "       'deathtime', 'admission_type', 'admission_location',\n",
      "       'discharge_location', 'insurance', 'language', 'religion',\n",
      "       'marital_status', 'ethnicity', 'edregtime', 'edouttime', 'diagnosis',\n",
      "       'hospital_expire_flag', 'has_chartevents_data'],\n",
      "      dtype='object')\n",
      "cptevents: Index(['row_id', 'subject_id', 'hadm_id', 'costcenter', 'chartdate', 'cpt_cd',\n",
      "       'cpt_number', 'cpt_suffix', 'ticket_id_seq', 'sectionheader',\n",
      "       'subsectionheader', 'description'],\n",
      "      dtype='object')\n",
      "d_labitems: Index(['row_id', 'itemid', 'label', 'fluid', 'category', 'loinc_code'], dtype='object')\n",
      "diagnoses_icd: Index(['row_id', 'subject_id', 'hadm_id', 'seq_num', 'icd9_code'], dtype='object')\n",
      "drgcodes: Index(['row_id', 'subject_id', 'hadm_id', 'drg_type', 'drg_code',\n",
      "       'description', 'drg_severity', 'drg_mortality'],\n",
      "      dtype='object')\n",
      "labevents: Index(['row_id', 'subject_id', 'hadm_id', 'itemid', 'charttime', 'value',\n",
      "       'valuenum', 'valueuom', 'flag'],\n",
      "      dtype='object')\n",
      "patients: Index(['row_id', 'subject_id', 'gender', 'dob', 'dod', 'dod_hosp', 'dod_ssn',\n",
      "       'expire_flag'],\n",
      "      dtype='object')\n",
      "procedures_icd: Index(['row_id', 'subject_id', 'hadm_id', 'seq_num', 'icd9_code'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('admissions:', admissions.columns)\n",
    "print('cptevents:', cptevents.columns)\n",
    "print('d_labitems:', d_labitems.columns)\n",
    "print('diagnoses_icd:', diagnoses_icd.columns)\n",
    "print('drgcodes:', drgcodes.columns)\n",
    "print('labevents:', labevents.columns)\n",
    "print('patients:', patients.columns)\n",
    "print('procedures_icd:', procedures_icd.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72092f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_codes = [\n",
    "    '39891','40201','40211','40291','40401','40403','40411','40413',\n",
    "    '40491','40493','4280','4281','42820','42821','42822','42823',\n",
    "    '42830','42831','42832','42833','42840','42841','42842','42843','4289'\n",
    "]\n",
    "\n",
    "# Step 1: Filter heart failure diagnoses and merge with admissions\n",
    "hf_diagnoses = diagnoses_icd[diagnoses_icd['icd9_code'].isin(heart_codes)]\n",
    "hf_admissions = pd.merge(\n",
    "    hf_diagnoses[['subject_id', 'hadm_id']],\n",
    "    admissions[['hadm_id', 'subject_id', 'admittime', 'dischtime', 'deathtime', \n",
    "                'admission_type', 'discharge_location', 'insurance', \n",
    "                'marital_status', 'ethnicity', 'hospital_expire_flag']],\n",
    "    on=['subject_id', 'hadm_id'],\n",
    "    how='left'\n",
    ").drop_duplicates(['subject_id', 'hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f032729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Convert date columns to datetime\n",
    "hf_admissions['admittime'] = pd.to_datetime(hf_admissions['admittime'])\n",
    "hf_admissions['dischtime'] = pd.to_datetime(hf_admissions['dischtime'])\n",
    "hf_admissions['deathtime'] = pd.to_datetime(hf_admissions['deathtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define 30-day readmission target\n",
    "hf_admissions_sorted = hf_admissions.sort_values(['subject_id', 'dischtime'])\n",
    "hf_admissions_sorted['next_admittime'] = hf_admissions_sorted.groupby('subject_id')['admittime'].shift(-1)\n",
    "hf_admissions_sorted['days_to_readmit'] = (\n",
    "    (hf_admissions_sorted['next_admittime'] - hf_admissions_sorted['dischtime']).dt.total_seconds() / 86400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "339daf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target: 1 if readmitted in â‰¤30 days (exclude deaths/transfers)\n",
    "hf_admissions_sorted['readmit_30'] = (\n",
    "    (hf_admissions_sorted['days_to_readmit'] <= 30) & \n",
    "    (hf_admissions_sorted['days_to_readmit'] > 0) &\n",
    "    (hf_admissions_sorted['deathtime'].isna()) &\n",
    "    (~hf_admissions_sorted['discharge_location'].isin(['DIED', 'HOSPICE']))\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8e346b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with patients table for age/gender\n",
    "hf_data = pd.merge(\n",
    "    hf_admissions_sorted,\n",
    "    patients[['subject_id', 'gender', 'dob']],\n",
    "    on='subject_id',\n",
    "    how='left'\n",
    ")\n",
    "hf_data['age'] = (hf_data['admittime'].dt.year - pd.to_datetime(hf_data['dob']).dt.year)\n",
    "hf_data['length_of_stay'] = (hf_data['dischtime'] - hf_data['admittime']).dt.total_seconds() / 86400\n",
    "\n",
    "# One-hot encode categoricals\n",
    "categorical_cols = ['gender', 'admission_type', 'insurance', 'marital_status', 'ethnicity']\n",
    "hf_data = pd.get_dummies(hf_data, columns=categorical_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "65d4c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on key labs (merge with d_labitems for names)\n",
    "lab_features = labevents.merge(\n",
    "    d_labitems[d_labitems['label'].str.contains('BNP|Creatinine|Sodium|Potassium', case=False)],\n",
    "    on='itemid'\n",
    ")\n",
    "\n",
    "# Pivot to get summary stats per admission\n",
    "lab_summary = lab_features.groupby(['hadm_id', 'label'])['valuenum'].agg(['mean', 'max']).unstack()\n",
    "lab_summary.columns = [f'{col[1]}_{col[0]}' for col in lab_summary.columns]  # Flatten multi-index\n",
    "\n",
    "# Merge with main data\n",
    "hf_data = hf_data.merge(lab_summary, on='hadm_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14ef0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- (1) Comorbidity Count (Elixhauser approximation) ----\n",
    "# Ensure icd9_code is string type first\n",
    "diagnoses_icd['icd9_code'] = diagnoses_icd['icd9_code'].astype(str)\n",
    "\n",
    "# Count unique diagnoses per admission (excluding heart failure codes to avoid leakage)\n",
    "non_hf_codes = diagnoses_icd[~diagnoses_icd['icd9_code'].isin(heart_codes)]\n",
    "comorbidities = non_hf_codes.groupby('hadm_id')['icd9_code'].nunique().rename('comorbidity_count')\n",
    "\n",
    "# Merge with main data\n",
    "hf_data = hf_data.merge(comorbidities, on='hadm_id', how='left').fillna({'comorbidity_count': 0})\n",
    "\n",
    "# ---- (2) Cardiac Procedure Flag ----\n",
    "# Convert icd9_code to string and handle missing values\n",
    "procedures_icd['icd9_code'] = procedures_icd['icd9_code'].astype(str)\n",
    "\n",
    "# Filter for cardiac procedures (ICD-9 codes starting with '37')\n",
    "cardiac_procedures = procedures_icd[\n",
    "    procedures_icd['icd9_code'].str.startswith('37', na=False)  # Explicitly handle NaN\n",
    "]\n",
    "\n",
    "# Create binary flag\n",
    "hf_data['had_cardiac_procedure'] = hf_data['hadm_id'].isin(\n",
    "    cardiac_procedures['hadm_id'].unique()\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "801d1c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using available features: ['age', 'length_of_stay', 'Creatinine_mean', 'Sodium_mean', 'had_cardiac_procedure', 'gender_M', 'admission_type_URGENT', 'insurance_Medicare', 'ethnicity_WHITE']\n",
      "Final columns: ['readmit_30', 'age', 'length_of_stay', 'Creatinine_mean', 'Sodium_mean', 'had_cardiac_procedure', 'gender_M', 'admission_type_URGENT', 'insurance_Medicare', 'ethnicity_WHITE']\n",
      "Class balance:\n",
      " readmit_30\n",
      "0    0.932072\n",
      "1    0.067928\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Identify available features (dynamic approach)\n",
    "available_features = [col for col in features if col in hf_data.columns]\n",
    "print(\"Using available features:\", available_features)\n",
    "\n",
    "# Handle missing columns\n",
    "# If BNP_mean is critical but missing, fetch it properly:\n",
    "if 'BNP_mean' not in hf_data.columns:\n",
    "    # Re-fetch BNP labs if missing\n",
    "    bnp_labs = labevents.merge(d_labitems[d_labitems['label'].str.contains('BNP', case=False)], \n",
    "                              on='itemid')\n",
    "    if not bnp_labs.empty:\n",
    "        bnp_mean = bnp_labs.groupby('hadm_id')['valuenum'].mean().rename('BNP_mean')\n",
    "        hf_data = hf_data.merge(bnp_mean, on='hadm_id', how='left')\n",
    "    else:\n",
    "        print(\"Warning: No BNP data found in labevents\")\n",
    "        hf_data['BNP_mean'] = np.nan  # Add as missing\n",
    "\n",
    "# Ensure comorbidity_count exists\n",
    "if 'comorbidity_count' not in hf_data.columns:\n",
    "    hf_data['comorbidity_count'] = 0  # Default to 0 if missing\n",
    "\n",
    "# Final selection with fallbacks\n",
    "required_cols = [target] + available_features\n",
    "hf_final = hf_data.dropna(subset=required_cols)[required_cols]\n",
    "\n",
    "# Verify\n",
    "print(\"Final columns:\", hf_final.columns.tolist())\n",
    "print(\"Class balance:\\n\", hf_final[target].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ffb0bb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.1-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\adity\\.conda\\envs\\tf-gpu\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\adity\\.conda\\envs\\tf-gpu\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\adity\\.conda\\envs\\tf-gpu\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\adity\\.conda\\envs\\tf-gpu\\lib\\site-packages (from xgboost) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\adity\\.conda\\envs\\tf-gpu\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\adity\\.conda\\envs\\tf-gpu\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\adity\\.conda\\envs\\tf-gpu\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adity\\.conda\\envs\\tf-gpu\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\adity\\.conda\\envs\\tf-gpu\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adity\\.conda\\envs\\tf-gpu\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading xgboost-3.0.1-py3-none-win_amd64.whl (150.0 MB)\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 18.1/150.0 MB 94.8 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 20.2/150.0 MB 55.5 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 21.2/150.0 MB 35.3 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 23.9/150.0 MB 28.5 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 27.5/150.0 MB 26.4 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 33.0/150.0 MB 26.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 40.1/150.0 MB 27.1 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 48.5/150.0 MB 28.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 54.3/150.0 MB 28.8 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 56.6/150.0 MB 26.9 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 60.3/150.0 MB 26.1 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 65.5/150.0 MB 26.1 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 72.4/150.0 MB 26.5 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 78.4/150.0 MB 26.7 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 82.1/150.0 MB 26.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 87.3/150.0 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 94.1/150.0 MB 26.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 98.6/150.0 MB 26.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 103.8/150.0 MB 26.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 110.1/150.0 MB 26.1 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 114.8/150.0 MB 25.8 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 120.6/150.0 MB 25.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 128.2/150.0 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 131.6/150.0 MB 25.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 135.8/150.0 MB 25.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 141.8/150.0 MB 25.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  148.1/150.0 MB 25.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.9/150.0 MB 25.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.9/150.0 MB 25.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.9/150.0 MB 25.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  149.9/150.0 MB 25.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 150.0/150.0 MB 22.2 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install xgboost scikit-learn pandas numpy\n",
    "# if not installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5a925dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-aucpr:0.07631\n",
      "[10]\tvalidation_0-aucpr:0.08435\n",
      "[20]\tvalidation_0-aucpr:0.08207\n",
      "[30]\tvalidation_0-aucpr:0.08102\n",
      "\n",
      "Classification Report (Default Threshold=0.5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.38      0.54      3887\n",
      "           1       0.08      0.70      0.14       283\n",
      "\n",
      "    accuracy                           0.40      4170\n",
      "   macro avg       0.51      0.54      0.34      4170\n",
      "weighted avg       0.89      0.40      0.51      4170\n",
      "\n",
      "\n",
      "Sensitivity (Recall): 0.70\n",
      "Precision: 0.08\n",
      "PR-AUC: 0.08\n",
      "\n",
      "Classification Report (Adjusted Threshold=0.2):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      3887\n",
      "           1       0.07      1.00      0.13       283\n",
      "\n",
      "    accuracy                           0.07      4170\n",
      "   macro avg       0.03      0.50      0.06      4170\n",
      "weighted avg       0.00      0.07      0.01      4170\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adity\\.conda\\envs\\tf-gpu\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\adity\\.conda\\envs\\tf-gpu\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\adity\\.conda\\envs\\tf-gpu\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>had_cardiac_procedure</td>\n",
       "      <td>0.140426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>length_of_stay</td>\n",
       "      <td>0.138103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Creatinine_mean</td>\n",
       "      <td>0.129245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>0.128460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>insurance_Medicare</td>\n",
       "      <td>0.116728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sodium_mean</td>\n",
       "      <td>0.103665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gender_M</td>\n",
       "      <td>0.092746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>admission_type_URGENT</td>\n",
       "      <td>0.082261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ethnicity_WHITE</td>\n",
       "      <td>0.068366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature  importance\n",
       "4  had_cardiac_procedure    0.140426\n",
       "1         length_of_stay    0.138103\n",
       "2        Creatinine_mean    0.129245\n",
       "0                    age    0.128460\n",
       "7     insurance_Medicare    0.116728\n",
       "3            Sodium_mean    0.103665\n",
       "5               gender_M    0.092746\n",
       "6  admission_type_URGENT    0.082261\n",
       "8        ethnicity_WHITE    0.068366"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Prepare Data (using your existing hf_final DataFrame)\n",
    "X = hf_final.drop(columns=['readmit_30'])\n",
    "y = hf_final['readmit_30']\n",
    "\n",
    "# 2. Train-Test Split (stratified to preserve imbalance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# 3. Train XGBoost with Class Weighting\n",
    "# Note: scale_pos_weight is a manual override for extreme imbalance\n",
    "# Note: eval_metric is set to 'aucpr' for precision-recall curve\n",
    "model = XGBClassifier(\n",
    "    scale_pos_weight=15,  # Manual class weighting\n",
    "    eval_metric='aucpr',  # Optimize for precision-recall\n",
    "    max_depth=3,          # Shallower trees\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.5,\n",
    "    n_estimators=100,\n",
    "    early_stopping_rounds=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Train with validation \n",
    "# Note: Early stopping rounds to prevent overfitting\n",
    "# Note: Adjust the eval_metric as per your requirement\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],  # Validation data\n",
    "    verbose=10  # Show training progress\n",
    ")\n",
    "\n",
    "# 3. Adjust prediction threshold\n",
    "# y_pred_adjusted = (model.predict_proba(X_test)[:,1] > 0.2).astype(int)  # Lower threshold\n",
    "\n",
    "# 4. Evaluate\n",
    "print(\"\\nClassification Report (Default Threshold=0.5):\")\n",
    "print(classification_report(y_test, model.predict(X_test)))\n",
    "\n",
    "# Confusion Matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, model.predict(X_test)).ravel()\n",
    "print(f\"\\nSensitivity (Recall): {tp/(tp+fn):.2f}\")\n",
    "print(f\"Precision: {tp/(tp+fp):.2f}\")\n",
    "\n",
    "# PR-AUC\n",
    "precision, recall, _ = precision_recall_curve(y_test, model.predict_proba(X_test)[:,1])\n",
    "print(f\"PR-AUC: {auc(recall, precision):.2f}\")\n",
    "\n",
    "# 5. Adjust threshold for better recall\n",
    "y_pred_adjusted = (model.predict_proba(X_test)[:,1] > 0.2).astype(int)\n",
    "print(\"\\nClassification Report (Adjusted Threshold=0.2):\")\n",
    "print(classification_report(y_test, y_pred_adjusted))\n",
    "\n",
    "# 6. Feature Importance\n",
    "pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
