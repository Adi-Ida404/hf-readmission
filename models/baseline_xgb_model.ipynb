{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8257585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define file paths\n",
    "data_path = '../data/raw/readmission/'\n",
    "\n",
    "# Read CSV files into DataFrames\n",
    "admissions = pd.read_csv(data_path + 'admissions_202208161605.csv')\n",
    "d_labitems = pd.read_csv(data_path + 'd_labitems_202208161605.csv')\n",
    "diagnoses_icd = pd.read_csv(data_path + 'diagnoses_icd_202208161605.csv')\n",
    "labevents = pd.read_csv(data_path + 'labevents_202208161605.csv')\n",
    "patients = pd.read_csv(data_path + 'patients_202208161605.csv')\n",
    "procedures_icd = pd.read_csv(data_path + 'procedures_icd_202208161605.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd905201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heart failure ICD-9 codes\n",
    "heart_codes = [\n",
    "    '39891','40201','40211','40291','40401','40403','40411','40413',\n",
    "    '40491','40493','4280','4281','42820','42821','42822','42823',\n",
    "    '42830','42831','42832','42833','42840','42841','42842','42843','4289'\n",
    "]\n",
    "\n",
    "# Filter for HF diagnoses\n",
    "hf_patients = diagnoses_icd[diagnoses_icd['icd9_code'].isin(heart_codes)][['subject_id', 'hadm_id']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d4e45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with admissions data\n",
    "hf_data = hf_patients.merge(\n",
    "    admissions,\n",
    "    on=['subject_id', 'hadm_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Add patient demographics\n",
    "hf_data = hf_data.merge(\n",
    "    patients[['subject_id', 'gender', 'dob']],\n",
    "    on='subject_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate age\n",
    "hf_data['age'] = (pd.to_datetime(hf_data['admittime']).dt.year) - pd.to_datetime(hf_data['dob']).dt.year\n",
    "hf_data = hf_data[hf_data['age'] >= 18]  # Adults only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "750341d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by patient and admission time\n",
    "hf_data = hf_data.sort_values(['subject_id', 'admittime'])\n",
    "\n",
    "# Calculate next admission time\n",
    "hf_data['next_admittime'] = hf_data.groupby('subject_id')['admittime'].shift(-1)\n",
    "\n",
    "# Define target\n",
    "hf_data['readmit_30'] = (\n",
    "    (pd.to_datetime(hf_data['next_admittime']) - pd.to_datetime(hf_data['dischtime'])).dt.days <= 30\n",
    ").astype(int)\n",
    "\n",
    "# Exclude deaths\n",
    "hf_data = hf_data[hf_data['deathtime'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1954d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get key lab tests (BNP, Creatinine, Sodium)\n",
    "lab_features = labevents.merge(\n",
    "    d_labitems[d_labitems['label'].str.contains('BNP|Creatinine|Sodium', case=False)],\n",
    "    on='itemid'\n",
    ")\n",
    "\n",
    "# Calculate mean values per admission\n",
    "lab_means = lab_features.groupby(['hadm_id', 'label'])['valuenum'].mean().unstack()\n",
    "lab_means.columns = [f'{col}_mean' for col in lab_means.columns]\n",
    "\n",
    "# Merge with main data\n",
    "hf_data = hf_data.merge(lab_means, on='hadm_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6b35655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count non-HF diagnoses per admission\n",
    "comorbidities = diagnoses_icd[~diagnoses_icd['icd9_code'].isin(heart_codes)]\n",
    "hf_data['comorbidity_count'] = hf_data['hadm_id'].map(\n",
    "    comorbidities.groupby('hadm_id')['icd9_code'].nunique()\n",
    ").fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26fd5bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in hf_data: ['subject_id', 'hadm_id', 'row_id', 'admittime', 'dischtime', 'deathtime', 'admission_type', 'admission_location', 'discharge_location', 'insurance', 'language', 'religion', 'marital_status', 'ethnicity', 'edregtime', 'edouttime', 'diagnosis', 'hospital_expire_flag', 'has_chartevents_data', 'gender', 'dob', 'age', 'next_admittime', 'readmit_30', '24 hr Creatinine_mean', 'Albumin/Creatinine, Urine_mean', 'Amylase/Creatinine Ratio, Urine_mean', 'Creatinine_mean', 'Creatinine Clearance_mean', 'Creatinine, Ascites_mean', 'Creatinine, Body Fluid_mean', 'Creatinine, Joint Fluid_mean', 'Creatinine, Pleural_mean', 'Creatinine, Serum_mean', 'Creatinine, Urine_mean', 'NTproBNP_mean', 'Protein/Creatinine Ratio_mean', 'Sodium_mean', 'Sodium, Ascites_mean', 'Sodium, Body Fluid_mean', 'Sodium, Pleural_mean', 'Sodium, Stool_mean', 'Sodium, Urine_mean', 'Sodium, Whole Blood_mean', 'Urine Creatinine_mean', 'comorbidity_count']\n"
     ]
    }
   ],
   "source": [
    "print(\"Available columns in hf_data:\", hf_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90f816f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to fetch BNP data...\n",
      "BNP_mean added successfully\n"
     ]
    }
   ],
   "source": [
    "# Check if BNP data exists in labevents\n",
    "if 'BNP_mean' not in hf_data.columns:\n",
    "    print(\"Attempting to fetch BNP data...\")\n",
    "    \n",
    "    # Get BNP itemid(s)\n",
    "    bnp_items = d_labitems[d_labitems['label'].str.contains('BNP', case=False)]['itemid']\n",
    "    \n",
    "    if not bnp_items.empty:\n",
    "        # Calculate mean BNP per admission\n",
    "        bnp_values = labevents[labevents['itemid'].isin(bnp_items)]\n",
    "        bnp_means = bnp_values.groupby('hadm_id')['valuenum'].mean().rename('BNP_mean')\n",
    "        hf_data = hf_data.merge(bnp_means, on='hadm_id', how='left')\n",
    "        print(\"BNP_mean added successfully\")\n",
    "    else:\n",
    "        print(\"Warning: No BNP tests found in d_labitems\")\n",
    "        hf_data['BNP_mean'] = np.nan  # Add as missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cbba360",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'comorbidity_count' not in hf_data.columns:\n",
    "    print(\"Recalculating comorbidity count...\")\n",
    "    \n",
    "    # Count non-HF diagnoses\n",
    "    comorbidities = diagnoses_icd[~diagnoses_icd['icd9_code'].isin(heart_codes)]\n",
    "    comorbidities = comorbidities[comorbidities['hadm_id'].isin(hf_data['hadm_id'])]\n",
    "    comorbidity_counts = comorbidities.groupby('hadm_id')['icd9_code'].nunique().rename('comorbidity_count')\n",
    "    \n",
    "    hf_data = hf_data.merge(comorbidity_counts, on='hadm_id', how='left').fillna({'comorbidity_count': 0})\n",
    "    print(\"comorbidity_count added successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99212637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final features being used: ['age', 'gender', 'admission_type', 'insurance', 'Creatinine_mean', 'Sodium_mean', 'BNP_mean', 'comorbidity_count']\n"
     ]
    }
   ],
   "source": [
    "# Start with guaranteed basic features\n",
    "features = ['age', 'gender', 'admission_type', 'insurance']\n",
    "\n",
    "# Add available lab features\n",
    "lab_features = ['Creatinine_mean', 'Sodium_mean', 'BNP_mean']\n",
    "features += [f for f in lab_features if f in hf_data.columns]\n",
    "\n",
    "# Add comorbidity count if available\n",
    "if 'comorbidity_count' in hf_data.columns:\n",
    "    features.append('comorbidity_count')\n",
    "\n",
    "print(\"Final features being used:\", features)\n",
    "\n",
    "# Create final dataset\n",
    "X = pd.get_dummies(hf_data[features], drop_first=True)\n",
    "y = hf_data['readmit_30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95234c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per feature:\n",
      "age                            0\n",
      "Creatinine_mean              124\n",
      "Sodium_mean                  126\n",
      "BNP_mean                    9574\n",
      "comorbidity_count              0\n",
      "gender_M                       0\n",
      "admission_type_EMERGENCY       0\n",
      "admission_type_URGENT          0\n",
      "insurance_Medicaid             0\n",
      "insurance_Medicare             0\n",
      "insurance_Private              0\n",
      "insurance_Self Pay             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values per feature:\")\n",
    "print(X.isna().sum())\n",
    "\n",
    "# Simple imputation (adjust as needed)\n",
    "if X.isna().any().any():\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f127f24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Define feature groups for different imputation strategies\n",
    "vital_features = ['age', 'comorbidity_count']\n",
    "lab_features = ['Creatinine_mean', 'Sodium_mean', 'BNP_mean']\n",
    "categoricals = [col for col in X.columns if col.startswith(('gender_', 'admission_type_', 'insurance_'))]\n",
    "\n",
    "# Custom imputation: median for labs, 0 for BNP if >90% missing\n",
    "imputer = SimpleImputer(\n",
    "    strategy='median',\n",
    "    missing_values=np.nan,\n",
    "    add_indicator=True  # Adds binary columns indicating imputation\n",
    ")\n",
    "\n",
    "# If BNP is >90% missing, drop it and use creatinine as proxy\n",
    "if X['BNP_mean'].isna().mean() > 0.9:\n",
    "    print(\"Dropping BNP_mean due to excessive missingness\")\n",
    "    lab_features.remove('BNP_mean')\n",
    "    X.drop(columns=['BNP_mean'], inplace=True)\n",
    "\n",
    "# Scale only continuous features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('vitals', 'passthrough', vital_features),\n",
    "        ('labs', make_pipeline(imputer, StandardScaler()), lab_features),\n",
    "        ('cats', 'passthrough', categoricals)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af53c81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in y: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for consistent lengths\n",
    "assert len(X) == len(y), f\"X has {len(X)} samples, y has {len(y)}\"\n",
    "\n",
    "# Check for NaN in target\n",
    "print(f\"Missing values in y: {y.isna().sum()}\")\n",
    "\n",
    "# Drop any rows with NaN in target if needed\n",
    "if y.isna().any():\n",
    "    y = y.dropna()\n",
    "    X = X.loc[y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9440693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error: Specifying the columns using strings is only supported for dataframes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Define feature groups\n",
    "numeric_features = ['age', 'Creatinine_mean', 'Sodium_mean', 'comorbidity_count']\n",
    "categorical_features = [col for col in X.columns if col.startswith(('gender_', 'admission_type_', 'insurance_'))]\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), numeric_features),\n",
    "        ('cat', 'passthrough', categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        class_weight='balanced_subsample',\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Verify pipeline\n",
    "from sklearn.utils import estimator_checks\n",
    "try:\n",
    "    estimator_checks.check_estimator(pipeline)\n",
    "    print(\"Pipeline validated successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Validation error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff3db1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance:\n",
      "readmit_30\n",
      "0    0.919223\n",
      "1    0.080777\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Mean values by class:\n",
      "              age  Creatinine_mean  Sodium_mean     BNP_mean  \\\n",
      "target                                                         \n",
      "0       90.772377         1.702699   138.726474  6027.677798   \n",
      "1       87.805986         2.018073   138.554589  6941.520382   \n",
      "\n",
      "        comorbidity_count  gender_M  admission_type_EMERGENCY  \\\n",
      "target                                                          \n",
      "0               13.003718  0.530879                  0.847737   \n",
      "1               15.052632  0.546956                  0.929825   \n",
      "\n",
      "        admission_type_URGENT  insurance_Medicaid  insurance_Medicare  \\\n",
      "target                                                                  \n",
      "0                    0.031559            0.057677            0.738732   \n",
      "1                    0.020640            0.063983            0.794634   \n",
      "\n",
      "        insurance_Private  insurance_Self Pay  \n",
      "target                                         \n",
      "0                0.187086            0.002811  \n",
      "1                0.130031            0.003096  \n"
     ]
    }
   ],
   "source": [
    "# Verify class balance\n",
    "print(f\"Class balance:\\n{y.value_counts(normalize=True)}\")\n",
    "\n",
    "# Check for data leakage\n",
    "print(\"\\nMean values by class:\")\n",
    "print(X.assign(target=y).groupby('target').mean())\n",
    "\n",
    "# Ensure no missing values in target\n",
    "assert y.isna().sum() == 0, f\"Missing values in y: {y.isna().sum()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee426580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define feature types\n",
    "numeric_features = ['age', 'Creatinine_mean', 'Sodium_mean', 'comorbidity_count']\n",
    "categorical_features = [col for col in X.columns if col.startswith(('gender_', 'admission_type_', 'insurance_'))]\n",
    "\n",
    "# Create preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', make_pipeline(\n",
    "        SimpleImputer(strategy='median'),\n",
    "        StandardScaler()\n",
    "    ), numeric_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "])\n",
    "\n",
    "# Simplified XGBoost without early stopping first\n",
    "base_model = XGBClassifier(\n",
    "    scale_pos_weight=3,  # Less aggressive weighting\n",
    "    eval_metric='aucpr',\n",
    "    max_depth=3,         # Shallower trees\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Initial pipeline\n",
    "pipeline = make_pipeline(preprocessor, base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fc900d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-validation metrics:\n",
      "Precision: 0.21 ± 0.13\n",
      "Recall:    0.01 ± 0.01\n",
      "F1:        0.02 ± 0.01\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Basic cross-validation\n",
    "cv_results = cross_validate(\n",
    "    pipeline, X, y,\n",
    "    scoring=['precision', 'recall', 'f1'],\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nCross-validation metrics:\")\n",
    "print(f\"Precision: {cv_results['test_precision'].mean():.2f} ± {cv_results['test_precision'].std():.2f}\")\n",
    "print(f\"Recall:    {cv_results['test_recall'].mean():.2f} ± {cv_results['test_recall'].std():.2f}\")\n",
    "print(f\"F1:        {cv_results['test_f1'].mean():.2f} ± {cv_results['test_f1'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afb6befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: If features > 20, use RFE\n",
    "if len(numeric_features + categorical_features) > 20:\n",
    "    from sklearn.feature_selection import RFE\n",
    "    selector = RFE(\n",
    "        estimator=RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "        n_features_to_select=15\n",
    "    )\n",
    "# Option B: Otherwise use variance threshold\n",
    "else:\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    selector = VarianceThreshold(threshold=0.01)  # Remove near-constant features\n",
    "\n",
    "# Add to pipeline\n",
    "pipeline.steps.insert(1, ('feature_selection', selector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b270e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-aucpr:0.11440\n",
      "[10]\tvalidation_0-aucpr:0.12238\n",
      "[10]\tvalidation_0-aucpr:0.12238\n",
      "[20]\tvalidation_0-aucpr:0.11982\n",
      "[20]\tvalidation_0-aucpr:0.11982\n",
      "[30]\tvalidation_0-aucpr:0.12043\n",
      "[30]\tvalidation_0-aucpr:0.12043\n",
      "\n",
      "F1-optimized threshold: 0.234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.74      0.83      2206\n",
      "           1       0.13      0.46      0.21       194\n",
      "\n",
      "    accuracy                           0.72      2400\n",
      "   macro avg       0.54      0.60      0.52      2400\n",
      "weighted avg       0.87      0.72      0.78      2400\n",
      "\n",
      "                  feature  importance\n",
      "3  num__comorbidity_count    0.410726\n",
      "1    num__Creatinine_mean    0.219519\n",
      "0                num__age    0.185392\n",
      "2        num__Sodium_mean    0.184363\n",
      "\n",
      "F1-optimized threshold: 0.234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.74      0.83      2206\n",
      "           1       0.13      0.46      0.21       194\n",
      "\n",
      "    accuracy                           0.72      2400\n",
      "   macro avg       0.54      0.60      0.52      2400\n",
      "weighted avg       0.87      0.72      0.78      2400\n",
      "\n",
      "                  feature  importance\n",
      "3  num__comorbidity_count    0.410726\n",
      "1    num__Creatinine_mean    0.219519\n",
      "0                num__age    0.185392\n",
      "2        num__Sodium_mean    0.184363\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, classification_report\n",
    "\n",
    "# 1. Train-test split (already done above)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Fit preprocessor and feature selector on training data only\n",
    "X_train_prep = preprocessor.fit_transform(X_train)\n",
    "X_val_prep = preprocessor.transform(X_val)\n",
    "\n",
    "feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=50, random_state=42))\n",
    "feature_selector.fit(X_train_prep, y_train)\n",
    "\n",
    "X_train_fs = feature_selector.transform(X_train_prep)\n",
    "X_val_fs = feature_selector.transform(X_val_prep)\n",
    "\n",
    "# 3. Define and fit XGBoost with early stopping\n",
    "model = XGBClassifier(\n",
    "    scale_pos_weight=3,\n",
    "    eval_metric='aucpr',\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    n_estimators=200,\n",
    "    early_stopping_rounds=20,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(\n",
    "    X_train_fs, y_train,\n",
    "    eval_set=[(X_val_fs, y_val)],\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "# 4. Predict and evaluate\n",
    "y_proba = model.predict_proba(X_val_fs)[:, 1]\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_proba)\n",
    "# Avoid division by zero in F1 calculation\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "    f1_scores = np.nan_to_num(f1_scores, nan=0.0)  # Replace NaN with 0\n",
    "\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"\\nF1-optimized threshold: {optimal_threshold:.3f}\")\n",
    "y_pred_optimal = (y_proba > optimal_threshold).astype(int)\n",
    "print(classification_report(y_val, y_pred_optimal, zero_division=0))\n",
    "\n",
    "# 5. Feature importances (with feature names)\n",
    "try:\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "    selected_features = feature_names[feature_selector.get_support()]\n",
    "except Exception as e:\n",
    "    print(f\"Could not extract feature names: {e}\")\n",
    "    selected_features = [f\"Feature {i}\" for i in range(len(model.feature_importances_))]\n",
    "\n",
    "importances = model.feature_importances_\n",
    "importances_df = pd.DataFrame({\n",
    "    'feature': selected_features,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(importances_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
